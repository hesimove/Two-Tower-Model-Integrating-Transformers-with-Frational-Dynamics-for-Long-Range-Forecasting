{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69117a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1/7\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt  # 如需画 test loss 曲线会用到\n",
    "\n",
    "# === 可复现：固定到最大程度 ===\n",
    "SEED = 42\n",
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True    # 固定 cuDNN 算法\n",
    "    cudnn.benchmark = False       # 禁止“最优算法搜索”，避免非确定性\n",
    "set_global_seed(SEED)\n",
    "\n",
    "# Root path to UCRArchive_2018\n",
    "ROOT = r\".....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9290119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2/7:\n",
    "def clean_and_pad_timeseries(raw_2d, min_len=8, cap_len=None, pad_value=0.0,\n",
    "                             per_sample_standardize=True, fixed_len=None):\n",
    "    \"\"\"\n",
    "    Clean time series with tail NaNs, z-score per-sample (optional), and pad/clip.\n",
    "\n",
    "    Priority of output length:\n",
    "      1) fixed_len: if not None, output length = fixed_len (force)\n",
    "      2) cap_len:   if not None, output length = min(max_real_len, cap_len)\n",
    "      3) otherwise, output length = max_real_len of this input batch\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    N, T = raw_2d.shape\n",
    "    rows, keep_idx, real_lens = [], [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        row = raw_2d[i]\n",
    "        valid_vals = row[~np.isnan(row)]\n",
    "        L = valid_vals.shape[0]\n",
    "        if L < min_len:\n",
    "            continue\n",
    "        if per_sample_standardize:\n",
    "            mu = valid_vals.mean()\n",
    "            sigma = valid_vals.std()\n",
    "            valid_vals = (valid_vals - mu) / (sigma if sigma > 0 else 1.0)\n",
    "        rows.append(valid_vals); keep_idx.append(i); real_lens.append(L)\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        raise ValueError(\"All samples filtered out. Lower min_len if needed.\")\n",
    "\n",
    "    max_real_len = max(real_lens)\n",
    "    if fixed_len is not None:\n",
    "        target_len = int(fixed_len)\n",
    "    elif cap_len is not None:\n",
    "        target_len = min(max_real_len, cap_len)\n",
    "    else:\n",
    "        target_len = max_real_len\n",
    "\n",
    "    out = []\n",
    "    for arr in rows:\n",
    "        if arr.shape[0] >= target_len:\n",
    "            arr = arr[:target_len]\n",
    "        else:\n",
    "            arr = np.pad(arr, (0, target_len - arr.shape[0]), constant_values=pad_value)\n",
    "        out.append(arr)\n",
    "\n",
    "    X = np.stack(out, axis=0).astype(\"float32\")\n",
    "    return X, np.array(keep_idx, dtype=np.int64), np.array(real_lens, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b22af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3/7:\n",
    "class TwoTowerTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-tower Transformer:\n",
    "      - Tower 1: time series tokens [B, T, 1] -> embed -> transformer\n",
    "      - Tower 2: Aout vector [B, F] as a single token -> embed -> transformer\n",
    "      - Concat tokens -> final transformer -> flatten -> FC for multi-class logits\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim1, input_dim2,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3,\n",
    "                 num_heads, num_layers, num_classes,\n",
    "                 seq_len1, seq_len2):\n",
    "        super().__init__()\n",
    "        # To keep it simple we force equal hidden dims and divisibility by nhead\n",
    "        assert hidden_dim1 == hidden_dim2 == hidden_dim3, \"hidden dims must be equal in this version.\"\n",
    "        for h in (hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "            assert h % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding1 = nn.Linear(input_dim1, hidden_dim1)\n",
    "        self.embedding2 = nn.Linear(input_dim2, hidden_dim2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.transformer1 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim1, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.transformer2 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim2, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.final_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim3, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.seq_len1 = seq_len1\n",
    "        self.seq_len2 = 1  # treat Aout as a single token\n",
    "        fc_in = hidden_dim3 * (seq_len1 + self.seq_len2)\n",
    "        self.fc = nn.Linear(fc_in, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: [B, T, input_dim1], x2: [B, F] or [B, 1, F]\n",
    "        if x1.dim() == 2:\n",
    "            x1 = x1.unsqueeze(-1)\n",
    "        x1 = self.relu(self.embedding1(x1))\n",
    "        x1 = self.transformer1(x1)\n",
    "\n",
    "        if x2.dim() == 3:\n",
    "            assert x2.size(1) == 1, \"Expect x2 with L2=1 if 3D\"\n",
    "            x2 = x2.squeeze(1)\n",
    "        x2 = self.relu(self.embedding2(x2))\n",
    "        x2 = x2.unsqueeze(1)\n",
    "        x2 = self.transformer2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.final_transformer(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class VisitDataset(Dataset):\n",
    "    \"\"\"Simple tensor dataset for (visit time series, aout features, one-hot labels).\"\"\"\n",
    "    def __init__(self, visit_x, aout_x, y):\n",
    "        self.visit_x = visit_x.astype(\"float32\")\n",
    "        self.aout_x  = aout_x.astype(\"float32\")\n",
    "        self.y       = y.astype(\"float32\")\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.visit_x[idx], self.aout_x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c4eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 4/8: run_one_dataset (保存 final_for_test.pt + best_train.pt + metrics.json；不画图)\n",
    "def run_one_dataset(dataset_dir, dataset_name,\n",
    "                    device='cuda:0',\n",
    "                    batch_size=8, num_epochs=100, lr=1e-4,\n",
    "                    cap_len=None, patience=15):\n",
    "    import os, json, copy\n",
    "    import numpy as np, pandas as pd\n",
    "    import torch, torch.nn as nn, torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "    # ---------- paths ----------\n",
    "    tsv_train_path = os.path.join(dataset_dir, f\"{dataset_name}_TRAIN_cleaned.tsv\")\n",
    "    tsv_test_path  = os.path.join(dataset_dir, f\"{dataset_name}_TEST_cleaned.tsv\")\n",
    "    aout_train_csv = os.path.join(dataset_dir, f\"{dataset_name}_Aout_train_k2.csv\")\n",
    "    aout_test_csv  = os.path.join(dataset_dir, f\"{dataset_name}_Aout_test_k2.csv\")\n",
    "    if not all(os.path.exists(p) for p in [tsv_train_path, tsv_test_path, aout_train_csv, aout_test_csv]):\n",
    "        print(f\"⚠ Skip {dataset_name}, missing files\")\n",
    "        return None\n",
    "\n",
    "    # ---------- read (OFFICIAL split) ----------\n",
    "    tsv_tr = pd.read_csv(tsv_train_path, sep=\"\\t\", header=None)\n",
    "    tsv_te = pd.read_csv(tsv_test_path,  sep=\"\\t\", header=None)\n",
    "    csv_tr = pd.read_csv(aout_train_csv, header=None)\n",
    "    csv_te = pd.read_csv(aout_test_csv,  header=None)\n",
    "\n",
    "    y_tr_raw = tsv_tr.iloc[:,0].values\n",
    "    y_te_raw = tsv_te.iloc[:,0].values\n",
    "    visit_tr_raw = tsv_tr.iloc[:,1:].values.astype(\"float32\")\n",
    "    visit_te_raw = tsv_te.iloc[:,1:].values.astype(\"float32\")\n",
    "    aout_tr_raw_all  = csv_tr.iloc[:,1:].values.astype(\"float32\")\n",
    "    aout_te_raw_all  = csv_te.iloc[:,1:].values.astype(\"float32\")\n",
    "\n",
    "    # ---------- length from TRAIN ----------\n",
    "    tmp_train_clean, keep_tr0, _ = clean_and_pad_timeseries(\n",
    "        visit_tr_raw, min_len=8, cap_len=cap_len, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=None\n",
    "    )\n",
    "    train_seq_len = tmp_train_clean.shape[1]\n",
    "\n",
    "    visit_tr_clean, keep_tr, _ = clean_and_pad_timeseries(\n",
    "        visit_tr_raw, min_len=8, cap_len=cap_len, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=train_seq_len\n",
    "    )\n",
    "    y_tr = y_tr_raw[keep_tr]\n",
    "    aout_tr_raw = aout_tr_raw_all[keep_tr]\n",
    "\n",
    "    visit_te_clean, keep_te, _ = clean_and_pad_timeseries(\n",
    "        visit_te_raw, min_len=8, cap_len=cap_len, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=train_seq_len\n",
    "    )\n",
    "    y_te = y_te_raw[keep_te]\n",
    "    aout_te_raw = aout_te_raw_all[keep_te]\n",
    "\n",
    "    # ---------- scaler on TRAIN ----------\n",
    "    scaler = StandardScaler().fit(aout_tr_raw)\n",
    "    aout_tr = scaler.transform(aout_tr_raw).astype(\"float32\")\n",
    "    aout_te = scaler.transform(aout_te_raw).astype(\"float32\")\n",
    "\n",
    "    # ---------- labels ----------\n",
    "    classes = sorted(np.unique(y_tr))\n",
    "    Y_tr = label_binarize(y_tr, classes=classes).astype(\"float32\")\n",
    "    Y_te = label_binarize(y_te, classes=classes).astype(\"float32\")\n",
    "    num_classes = Y_tr.shape[1]\n",
    "\n",
    "    # targets & loss\n",
    "    if num_classes == 1:\n",
    "        y_tr_tgt = torch.from_numpy(Y_tr)                         # [N,1] float\n",
    "        y_te_tgt = torch.from_numpy(Y_te)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        y_tr_tgt = torch.from_numpy(np.argmax(Y_tr, axis=1).astype(np.int64))  # [N] long\n",
    "        y_te_tgt = torch.from_numpy(np.argmax(Y_te, axis=1).astype(np.int64))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ---------- assemble inputs ----------\n",
    "    visit_tr = visit_tr_clean[:, :, None]  # [N,L,1]\n",
    "    visit_te = visit_te_clean[:, :, None]\n",
    "    seq_len1 = visit_tr.shape[1]\n",
    "    input_dim1 = visit_tr.shape[2]\n",
    "    input_dim2 = aout_tr.shape[1]\n",
    "\n",
    "    # ---------- model ----------\n",
    "    dev = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = TwoTowerTransformer(\n",
    "        input_dim1, input_dim2,\n",
    "        hidden_dim1=16, hidden_dim2=16, hidden_dim3=16,\n",
    "        num_heads=2, num_layers=2,\n",
    "        num_classes=num_classes,\n",
    "        seq_len1=seq_len1, seq_len2=1\n",
    "    ).to(dev)\n",
    "    if torch.cuda.device_count() > 1 and str(dev).startswith('cuda'):\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # dataset/dataloader\n",
    "    class VisitDatasetForTrain(Dataset):\n",
    "        def __init__(self, visit_x, aout_x, tgt_tensor):\n",
    "            self.visit_x = visit_x.astype(\"float32\")\n",
    "            self.aout_x  = aout_x.astype(\"float32\")\n",
    "            self.tgt     = tgt_tensor\n",
    "        def __len__(self): return len(self.tgt)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.visit_x[idx], self.aout_x[idx], self.tgt[idx]\n",
    "\n",
    "    g = torch.Generator(device=\"cpu\").manual_seed(SEED)\n",
    "    train_loader = DataLoader(VisitDatasetForTrain(visit_tr, aout_tr, y_tr_tgt),\n",
    "                              batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True,\n",
    "                              worker_init_fn=lambda wid: np.random.seed(SEED + wid),\n",
    "                              generator=g)\n",
    "    test_loader  = DataLoader(VisitDatasetForTrain(visit_te, aout_te, y_te_tgt),\n",
    "                              batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # optimizer/scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0)\n",
    "\n",
    "    # early stopping on TRAIN loss\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=15, delta=0.0):\n",
    "            self.patience=patience; self.delta=delta\n",
    "            self.best=None; self.count=0; self.stop=False\n",
    "        def step(self, loss):\n",
    "            if self.best is None: self.best=loss; return False\n",
    "            if loss > self.best - self.delta:\n",
    "                self.count += 1\n",
    "                if self.count >= self.patience: self.stop=True\n",
    "            else:\n",
    "                self.best = loss; self.count = 0\n",
    "            return self.stop\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # 追踪 train-loss 最优权重\n",
    "    def _sd(m): return m.module.state_dict() if isinstance(m, nn.DataParallel) else m.state_dict()\n",
    "    best_train_loss = float(\"inf\")\n",
    "    best_train_state = None\n",
    "\n",
    "    # ---------- train ----------\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x1,x2,y in train_loader:\n",
    "            x1 = x1.to(torch.float32).to(dev)\n",
    "            x2 = x2.to(torch.float32).to(dev)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if num_classes==1:\n",
    "                y = y.to(torch.float32).to(dev)\n",
    "            else:\n",
    "                y = y.to(torch.long).to(dev)\n",
    "\n",
    "            logits = model(x1,x2)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward(); optimizer.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total / max(1, len(train_loader))\n",
    "\n",
    "        if avg_train_loss < best_train_loss:\n",
    "            best_train_loss = avg_train_loss\n",
    "            best_train_state = copy.deepcopy(_sd(model))\n",
    "\n",
    "        if es.step(avg_train_loss):\n",
    "            break\n",
    "\n",
    "    # ---------- TEST（一次性评估） ----------\n",
    "    model.eval()\n",
    "    test_total = 0.0\n",
    "    logits_list, tgt_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,y in test_loader:\n",
    "            x1 = x1.to(torch.float32).to(dev)\n",
    "            x2 = x2.to(torch.float32).to(dev)\n",
    "            if num_classes==1:\n",
    "                y = y.to(torch.float32).to(dev)\n",
    "            else:\n",
    "                y = y.to(torch.long).to(dev)\n",
    "\n",
    "            logits = model(x1,x2)\n",
    "            tloss = criterion(logits, y)\n",
    "            test_total += tloss.item()\n",
    "            logits_list.append(logits.detach().cpu().numpy())\n",
    "            tgt_list.append(y.detach().cpu().numpy())\n",
    "    test_loss = test_total / max(1, len(test_loader))\n",
    "\n",
    "    logits = np.concatenate(logits_list, axis=0)\n",
    "    tgt    = np.concatenate(tgt_list, axis=0)\n",
    "\n",
    "    if num_classes==1:\n",
    "        probs_pos = 1.0/(1.0+np.exp(-logits.ravel()))\n",
    "        y_true = tgt.ravel().astype(int)\n",
    "        y_pred = (probs_pos>=0.5).astype(int)\n",
    "        test_acc = accuracy_score(y_true, y_pred)\n",
    "        test_auc = roc_auc_score(y_true, probs_pos)\n",
    "    else:\n",
    "        ex = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "        probs = ex / ex.sum(axis=1, keepdims=True)\n",
    "        y_true = tgt.astype(int)\n",
    "        y_pred = probs.argmax(axis=1)\n",
    "        test_acc = accuracy_score(y_true, y_pred)\n",
    "        y_true_1h = np.eye(num_classes, dtype=int)[y_true]\n",
    "        test_auc = roc_auc_score(y_true_1h, probs, multi_class='ovr')\n",
    "\n",
    "    n_test = logits.shape[0]\n",
    "    print(f\"[{dataset_name}] TEST loss={test_loss:.4f} | AUC={test_auc:.4f} | ACC={test_acc:.4f} | n={n_test}\")\n",
    "\n",
    "    # ---------- 保存权重与指标 ----------\n",
    "    ckpt_dir = os.path.join(dataset_dir, \"_twotower_logs\"); os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    # (1) 最终用于测试的权重\n",
    "    final_path = os.path.join(ckpt_dir, f\"{dataset_name}_final_for_test.pt\")\n",
    "    torch.save({\n",
    "        \"model\": _sd(model),\n",
    "        \"scaler\": scaler,\n",
    "        \"classes\": classes,\n",
    "        \"seq_len\": int(seq_len1),\n",
    "        \"seed\": int(SEED),\n",
    "        \"epochs_run\": int(epoch+1),\n",
    "        \"hyper\": {\"hidden\":16, \"heads\":2, \"layers\":2, \"lr\":lr, \"batch\":batch_size}\n",
    "    }, final_path)\n",
    "\n",
    "    # (2) 训练期 train-loss 最优的权重\n",
    "    best_train_path = os.path.join(ckpt_dir, f\"{dataset_name}_best_train.pt\")\n",
    "    if best_train_state is not None:\n",
    "        torch.save({\n",
    "            \"model\": best_train_state,\n",
    "            \"scaler\": scaler,\n",
    "            \"classes\": classes,\n",
    "            \"seq_len\": int(seq_len1),\n",
    "            \"seed\": int(SEED),\n",
    "            \"best_train_loss\": float(best_train_loss),\n",
    "            \"hyper\": {\"hidden\":16, \"heads\":2, \"layers\":2, \"lr\":lr, \"batch\":batch_size}\n",
    "        }, best_train_path)\n",
    "\n",
    "    # (3) 指标 JSON（含权重路径）\n",
    "    with open(os.path.join(ckpt_dir, f\"{dataset_name}_metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"test_loss\": float(test_loss),\n",
    "            \"test_auc\": float(test_auc),\n",
    "            \"test_acc\": float(test_acc),\n",
    "            \"n_samples\": int(n_test),\n",
    "            \"epochs_run\": int(epoch+1),\n",
    "            \"seed\": int(SEED),\n",
    "            \"weights_final\": final_path,\n",
    "            \"weights_best_train\": best_train_path\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 统一 4 个返回值（注意顺序）\n",
    "    return float(test_auc), float(test_acc), float(test_loss), int(n_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3af89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5/7:\n",
    "def discover_datasets(root):\n",
    "    \"\"\"\n",
    "    Discover dataset subfolders that contain all four required files:\n",
    "      *_TRAIN_cleaned.tsv, *_TEST_cleaned.tsv, *_Aout_train_k2.csv, *_Aout_test_k2.csv\n",
    "    Returns: a sorted list of dataset names (folder names).\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    for name in sorted(os.listdir(root)):\n",
    "        subdir = os.path.join(root, name)\n",
    "        if not os.path.isdir(subdir):\n",
    "            continue\n",
    "        t_train = os.path.join(subdir, f\"{name}_TRAIN_cleaned.tsv\")\n",
    "        t_test  = os.path.join(subdir, f\"{name}_TEST_cleaned.tsv\")\n",
    "        a_train = os.path.join(subdir, f\"{name}_Aout_train_k2.csv\")\n",
    "        a_test  = os.path.join(subdir, f\"{name}_Aout_test_k2.csv\")\n",
    "        if all(os.path.exists(p) for p in [t_train, t_test, a_train, a_test]):\n",
    "            names.append(name)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715bc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 6/8: run all datasets（解包 4 个返回值；汇总里加 test_loss）\n",
    "def run_all_datasets(root, device='cuda:0',\n",
    "                     batch_size=8, num_epochs=100, lr=1e-4,\n",
    "                     cap_len=None, patience=15):\n",
    "    dataset_names = discover_datasets(root)\n",
    "    print(f\"Found {len(dataset_names)} datasets:\", dataset_names)\n",
    "\n",
    "    rows = []\n",
    "    for name in dataset_names:\n",
    "        out = run_one_dataset(\n",
    "            dataset_dir=os.path.join(root, name),\n",
    "            dataset_name=name,\n",
    "            device=device,\n",
    "            batch_size=batch_size, num_epochs=num_epochs, lr=lr,\n",
    "            cap_len=cap_len, patience=patience\n",
    "        )\n",
    "        if out is None:\n",
    "            continue\n",
    "        test_auc, test_acc, test_loss, n_test = out\n",
    "        rows.append((name, test_auc, test_acc, test_loss, n_test))\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No dataset finished successfully.\"); return None\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"dataset\",\"test_auc\",\"test_acc\",\"test_loss\",\"n_samples\"]).sort_values(\"dataset\")\n",
    "    mean_auc  = df[\"test_auc\"].mean()\n",
    "    mean_acc  = df[\"test_acc\"].mean()\n",
    "    mean_loss = df[\"test_loss\"].mean()\n",
    "    w_auc = (df[\"test_auc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "    w_acc = (df[\"test_acc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "\n",
    "    print(\"\\n========== Summary (OFFICIAL TEST) ==========\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSimple mean:  AUC = {mean_auc:.4f}, ACC = {mean_acc:.4f}, LOSS = {mean_loss:.4f}\")\n",
    "    print(f\"Weighted (by samples): AUC = {w_auc:.4f}, ACC = {w_acc:.4f}\")\n",
    "\n",
    "    summary_dir = os.path.join(root, \"_twotower_logs\"); os.makedirs(summary_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    df.to_csv(os.path.join(summary_dir, f\"summary_TEST_{ts}.csv\"), index=False, encoding=\"utf-8\")\n",
    "    with open(os.path.join(summary_dir, f\"summary_TEST_{ts}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df.to_string(index=False))\n",
    "        f.write(f\"\\n\\nSimple mean:  AUC={mean_auc:.6f}, ACC={mean_acc:.6f}, LOSS={mean_loss:.6f}\\n\")\n",
    "        f.write(f\"Weighted (by samples): AUC={w_auc:.6f}, ACC={w_acc:.6f}\\n\")\n",
    "\n",
    "    return df, (mean_auc, mean_acc, mean_loss), (w_auc, w_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0012851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ECG200] TEST loss=0.4102 | AUC=0.8950 | ACC=0.8300 | n=100\n",
      "[ECG200] AUC=0.8950 | ACC=0.8300 | LOSS=0.4102 | n=100\n",
      "[GunPoint] TEST loss=0.4334 | AUC=0.8718 | ACC=0.7467 | n=150\n",
      "[GunPoint] AUC=0.8718 | ACC=0.7467 | LOSS=0.4334 | n=150\n",
      "[ArrowHead] TEST loss=0.7773 | AUC=0.8708 | ACC=0.6400 | n=175\n",
      "[ArrowHead] AUC=0.8708 | ACC=0.6400 | LOSS=0.7773 | n=175\n",
      "[Beef] TEST loss=0.9264 | AUC=0.9153 | ACC=0.7333 | n=30\n",
      "[Beef] AUC=0.9153 | ACC=0.7333 | LOSS=0.9264 | n=30\n",
      "[Coffee] TEST loss=0.2095 | AUC=0.9897 | ACC=0.9286 | n=28\n",
      "[Coffee] AUC=0.9897 | ACC=0.9286 | LOSS=0.2095 | n=28\n",
      "[ECG5000] TEST loss=0.2410 | AUC=0.9329 | ACC=0.9367 | n=4500\n",
      "[ECG5000] AUC=0.9329 | ACC=0.9367 | LOSS=0.2410 | n=4500\n",
      "\n",
      "========== Summary (6 SELECTED | OFFICIAL TEST) ==========\n",
      "  dataset  test_auc  test_acc  test_loss  n_samples\n",
      "ArrowHead  0.870769  0.640000   0.777283        175\n",
      "     Beef  0.915278  0.733333   0.926377         30\n",
      "   Coffee  0.989744  0.928571   0.209466         28\n",
      "   ECG200  0.894965  0.830000   0.410186        100\n",
      "  ECG5000  0.932909  0.936667   0.240974       4500\n",
      " GunPoint  0.871799  0.746667   0.433373        150\n",
      "\n",
      "Simple mean:  AUC = 0.9126, ACC = 0.8025, LOSS = 0.4996\n",
      "Weighted (by samples): AUC = 0.9283, ACC = 0.9171\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 7/8: run ONLY the 6 selected datasets（解包 4 个返回值）\n",
    "SELECTED = [\"ECG200\", \"GunPoint\", \"ArrowHead\", \"Beef\", \"Coffee\", \"ECG5000\"]\n",
    "\n",
    "rows = []\n",
    "for ds in SELECTED:\n",
    "    ds_dir = os.path.join(ROOT, ds)\n",
    "    out = run_one_dataset(\n",
    "        dataset_dir=ds_dir,\n",
    "        dataset_name=ds,\n",
    "        device='cuda:0',\n",
    "        batch_size=8,\n",
    "        num_epochs=100,\n",
    "        lr=1e-4,\n",
    "        cap_len=None,\n",
    "        patience=15,\n",
    "    )\n",
    "    if out is None:\n",
    "        print(f\"[WARN] {ds} 缺少必要文件\"); continue\n",
    "    test_auc, test_acc, test_loss, test_n = out\n",
    "    rows.append((ds, float(test_auc), float(test_acc), float(test_loss), int(test_n)))\n",
    "    print(f\"[{ds}] AUC={test_auc:.4f} | ACC={test_acc:.4f} | LOSS={test_loss:.4f} | n={test_n}\")\n",
    "\n",
    "# 汇总（六个数据集）\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows, columns=[\"dataset\",\"test_auc\",\"test_acc\",\"test_loss\",\"n_samples\"]).sort_values(\"dataset\")\n",
    "    mean_auc = df[\"test_auc\"].mean(); mean_acc = df[\"test_acc\"].mean(); mean_loss = df[\"test_loss\"].mean()\n",
    "    w_auc = (df[\"test_auc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "    w_acc = (df[\"test_acc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "\n",
    "    print(\"\\n========== Summary (6 SELECTED | OFFICIAL TEST) ==========\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSimple mean:  AUC = {mean_auc:.4f}, ACC = {mean_acc:.4f}, LOSS = {mean_loss:.4f}\")\n",
    "    print(f\"Weighted (by samples): AUC = {w_auc:.4f}, ACC = {w_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No dataset finished successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff40b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 datasets: ['ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY', 'AllGestureWiimoteZ', 'ArrowHead', 'BME', 'Beef', 'BeetleFly', 'BirdChicken', 'CBF', 'Car', 'Chinatown', 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'CricketX', 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction', 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'ECG200', 'ECG5000', 'ECGFiveDays', 'EOGHorizontalSignal', 'EOGVerticalSignal', 'Earthquakes', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain', 'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2', 'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint', 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7', 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian', 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain', 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2', 'OSULeaf', 'OliveOil', 'PLAID', 'PhalangesOutlinesCorrect', 'Phoneme', 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineAgeGroup', 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n",
      "[ACSF1] TEST loss=0.8198 | AUC=0.9579 | ACC=0.7300 | n=100\n",
      "[Adiac] TEST loss=1.8234 | AUC=0.9384 | ACC=0.4655 | n=391\n",
      "[AllGestureWiimoteX] TEST loss=2.1228 | AUC=0.7975 | ACC=0.3950 | n=681\n",
      "[AllGestureWiimoteY] TEST loss=1.4746 | AUC=0.8772 | ACC=0.4837 | n=645\n",
      "[AllGestureWiimoteZ] TEST loss=2.0969 | AUC=0.8147 | ACC=0.3562 | n=685\n",
      "[ArrowHead] TEST loss=0.7663 | AUC=0.8438 | ACC=0.6343 | n=175\n",
      "[BME] TEST loss=0.5040 | AUC=0.9769 | ACC=0.7667 | n=150\n",
      "[Beef] TEST loss=0.9910 | AUC=0.8958 | ACC=0.6333 | n=30\n",
      "[BeetleFly] TEST loss=0.6819 | AUC=0.8900 | ACC=0.8000 | n=20\n",
      "[BirdChicken] TEST loss=0.5547 | AUC=0.7700 | ACC=0.8000 | n=20\n",
      "[CBF] TEST loss=0.2555 | AUC=0.9856 | ACC=0.9100 | n=900\n",
      "[Car] TEST loss=0.7956 | AUC=0.9132 | ACC=0.7833 | n=60\n",
      "[Chinatown] TEST loss=0.4865 | AUC=0.9615 | ACC=0.8397 | n=343\n",
      "[ChlorineConcentration] TEST loss=0.9278 | AUC=0.6729 | ACC=0.5669 | n=3840\n",
      "[CinCECGTorso] TEST loss=0.2296 | AUC=0.9942 | ACC=0.9130 | n=1380\n",
      "[Coffee] TEST loss=0.2899 | AUC=0.9795 | ACC=0.8929 | n=28\n",
      "[CricketX] TEST loss=1.8011 | AUC=0.8908 | ACC=0.5410 | n=390\n",
      "[CricketY] TEST loss=1.6869 | AUC=0.9107 | ACC=0.5462 | n=390\n",
      "[CricketZ] TEST loss=1.6051 | AUC=0.8860 | ACC=0.5538 | n=390\n",
      "[Crop] TEST loss=0.9221 | AUC=0.9714 | ACC=0.7028 | n=16778\n",
      "[DiatomSizeReduction] TEST loss=1.0165 | AUC=0.9117 | ACC=0.5294 | n=306\n",
      "[DistalPhalanxOutlineAgeGroup] TEST loss=0.6366 | AUC=0.8259 | ACC=0.6970 | n=99\n",
      "[DistalPhalanxOutlineCorrect] TEST loss=0.5044 | AUC=0.8201 | ACC=0.7538 | n=199\n",
      "[DistalPhalanxTW] TEST loss=1.0009 | AUC=0.8400 | ACC=0.5319 | n=94\n",
      "[DodgerLoopDay] TEST loss=1.1357 | AUC=0.8670 | ACC=0.5500 | n=80\n",
      "[DodgerLoopGame] TEST loss=0.5746 | AUC=0.7912 | ACC=0.6957 | n=138\n",
      "[DodgerLoopWeekend] TEST loss=0.1537 | AUC=0.9918 | ACC=0.9710 | n=138\n",
      "[ECG200] TEST loss=0.4025 | AUC=0.8958 | ACC=0.7900 | n=100\n",
      "[ECG5000] TEST loss=0.2273 | AUC=0.9450 | ACC=0.9391 | n=4500\n",
      "[ECGFiveDays] TEST loss=0.4560 | AUC=0.9176 | ACC=0.7782 | n=861\n",
      "[EOGHorizontalSignal] TEST loss=2.8986 | AUC=0.8595 | ACC=0.4254 | n=362\n",
      "[EOGVerticalSignal] TEST loss=4.9537 | AUC=0.8174 | ACC=0.3785 | n=362\n",
      "[Earthquakes] TEST loss=0.6273 | AUC=0.6761 | ACC=0.7626 | n=139\n",
      "[EthanolLevel] TEST loss=1.4079 | AUC=0.7464 | ACC=0.4540 | n=500\n",
      "[FaceAll] TEST loss=2.0567 | AUC=0.9582 | ACC=0.7408 | n=1690\n",
      "[FaceFour] TEST loss=0.5279 | AUC=0.9567 | ACC=0.8025 | n=81\n",
      "[FacesUCR] TEST loss=0.7467 | AUC=0.9698 | ACC=0.7946 | n=2050\n",
      "[FiftyWords] TEST loss=1.7367 | AUC=0.9260 | ACC=0.6505 | n=455\n",
      "[Fish] TEST loss=0.5996 | AUC=0.9703 | ACC=0.8057 | n=175\n",
      "[FordA] TEST loss=0.8533 | AUC=0.5703 | ACC=0.5439 | n=1320\n",
      "[FordB] TEST loss=0.8802 | AUC=0.5400 | ACC=0.5086 | n=810\n",
      "[FreezerRegularTrain] TEST loss=0.0837 | AUC=0.9903 | ACC=0.9846 | n=2850\n",
      "[FreezerSmallTrain] TEST loss=0.6827 | AUC=0.7457 | ACC=0.7158 | n=2850\n",
      "[Fungi] TEST loss=0.3993 | AUC=0.9985 | ACC=0.9462 | n=186\n",
      "[GestureMidAirD1] TEST loss=1.0471 | AUC=0.9708 | ACC=0.6077 | n=130\n",
      "[GestureMidAirD2] TEST loss=1.3120 | AUC=0.9625 | ACC=0.5923 | n=130\n",
      "[GestureMidAirD3] TEST loss=2.2936 | AUC=0.9208 | ACC=0.4231 | n=130\n",
      "[GesturePebbleZ1] TEST loss=0.7753 | AUC=0.9698 | ACC=0.8372 | n=172\n",
      "[GesturePebbleZ2] TEST loss=0.6246 | AUC=0.9713 | ACC=0.7975 | n=158\n",
      "[GunPoint] TEST loss=0.4392 | AUC=0.8725 | ACC=0.7933 | n=150\n",
      "[GunPointAgeSpan] TEST loss=0.1039 | AUC=0.9949 | ACC=0.9652 | n=316\n",
      "[GunPointMaleVersusFemale] TEST loss=0.0368 | AUC=0.9996 | ACC=0.9873 | n=316\n",
      "[GunPointOldVersusYoung] TEST loss=0.1623 | AUC=0.9880 | ACC=0.9460 | n=315\n",
      "[Ham] TEST loss=0.5967 | AUC=0.8152 | ACC=0.6952 | n=105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Cell 8/8: optional — run ALL datasets in ROOT\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcap_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mrun_all_datasets\u001b[1;34m(root, device, batch_size, num_epochs, lr, cap_len, patience)\u001b[0m\n\u001b[0;32m      9\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m dataset_names:\n\u001b[1;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcap_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcap_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 157\u001b[0m, in \u001b[0;36mrun_one_dataset\u001b[1;34m(dataset_dir, dataset_name, device, batch_size, num_epochs, lr, cap_len, patience)\u001b[0m\n\u001b[0;32m    155\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[0;32m    156\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(); optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 157\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    160\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 8/8: optional — run ALL datasets in ROOT\n",
    "_ = run_all_datasets(\n",
    "    root=ROOT,\n",
    "    device='cuda:0',\n",
    "    batch_size=8,\n",
    "    num_epochs=100,\n",
    "    lr=1e-4,\n",
    "    cap_len=None,\n",
    "    patience=15,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
