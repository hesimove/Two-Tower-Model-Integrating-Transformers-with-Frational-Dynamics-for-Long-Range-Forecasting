{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0162eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt  # NEW: for line charts\n",
    "\n",
    "# Global settings / reproducibility\n",
    "cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Root path to UCRArchive_2018\n",
    "ROOT = r\"D:\\2025暑期科研\\UCRArchive_2018\\UCRArchive_2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c86c0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "def clean_and_pad_timeseries(raw_2d, min_len=8, cap_len=None, pad_value=0.0,\n",
    "                             per_sample_standardize=True, fixed_len=None):\n",
    "    \"\"\"\n",
    "    Clean time series with tail NaNs, z-score per-sample (optional), and pad/clip.\n",
    "\n",
    "    Priority of output length:\n",
    "      1) fixed_len: if not None, output length = fixed_len (force)\n",
    "      2) cap_len:   if not None, output length = min(max_real_len, cap_len)\n",
    "      3) otherwise, output length = max_real_len of this input batch\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    N, T = raw_2d.shape\n",
    "    rows, keep_idx, real_lens = [], [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        row = raw_2d[i]\n",
    "        valid_vals = row[~np.isnan(row)]\n",
    "        L = valid_vals.shape[0]\n",
    "        if L < min_len:\n",
    "            continue\n",
    "        if per_sample_standardize:\n",
    "            mu = valid_vals.mean()\n",
    "            sigma = valid_vals.std()\n",
    "            valid_vals = (valid_vals - mu) / (sigma if sigma > 0 else 1.0)\n",
    "        rows.append(valid_vals); keep_idx.append(i); real_lens.append(L)\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        raise ValueError(\"All samples filtered out. Lower min_len if needed.\")\n",
    "\n",
    "    max_real_len = max(real_lens)\n",
    "    if fixed_len is not None:\n",
    "        target_len = int(fixed_len)\n",
    "    elif cap_len is not None:\n",
    "        target_len = min(max_real_len, cap_len)\n",
    "    else:\n",
    "        target_len = max_real_len\n",
    "\n",
    "    out = []\n",
    "    for arr in rows:\n",
    "        if arr.shape[0] >= target_len:\n",
    "            arr = arr[:target_len]\n",
    "        else:\n",
    "            arr = np.pad(arr, (0, target_len - arr.shape[0]), constant_values=pad_value)\n",
    "        out.append(arr)\n",
    "\n",
    "    X = np.stack(out, axis=0).astype(\"float32\")\n",
    "    return X, np.array(keep_idx, dtype=np.int64), np.array(real_lens, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7398227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "class TwoTowerTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-tower Transformer:\n",
    "      - Tower 1: time series tokens [B, T, 1] -> embed -> transformer\n",
    "      - Tower 2: Aout vector [B, F] as a single token -> embed -> transformer\n",
    "      - Concat tokens -> final transformer -> flatten -> FC for multi-class logits\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim1, input_dim2,\n",
    "                 hidden_dim1, hidden_dim2, hidden_dim3,\n",
    "                 num_heads, num_layers, num_classes,\n",
    "                 seq_len1, seq_len2):\n",
    "        super().__init__()\n",
    "        # To keep it simple we force equal hidden dims and divisibility by nhead\n",
    "        assert hidden_dim1 == hidden_dim2 == hidden_dim3, \"hidden dims must be equal in this version.\"\n",
    "        for h in (hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "            assert h % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.embedding1 = nn.Linear(input_dim1, hidden_dim1)\n",
    "        self.embedding2 = nn.Linear(input_dim2, hidden_dim2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.transformer1 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim1, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.transformer2 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim2, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.final_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim3, nhead=num_heads, batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.seq_len1 = seq_len1\n",
    "        self.seq_len2 = 1  # treat Aout as a single token\n",
    "        fc_in = hidden_dim3 * (seq_len1 + self.seq_len2)\n",
    "        self.fc = nn.Linear(fc_in, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: [B, T, input_dim1], x2: [B, F] or [B, 1, F]\n",
    "        if x1.dim() == 2:\n",
    "            x1 = x1.unsqueeze(-1)\n",
    "        x1 = self.relu(self.embedding1(x1))\n",
    "        x1 = self.transformer1(x1)\n",
    "\n",
    "        if x2.dim() == 3:\n",
    "            assert x2.size(1) == 1, \"Expect x2 with L2=1 if 3D\"\n",
    "            x2 = x2.squeeze(1)\n",
    "        x2 = self.relu(self.embedding2(x2))\n",
    "        x2 = x2.unsqueeze(1)\n",
    "        x2 = self.transformer2(x2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.final_transformer(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class VisitDataset(Dataset):\n",
    "    \"\"\"Simple tensor dataset for (visit time series, aout features, one-hot labels).\"\"\"\n",
    "    def __init__(self, visit_x, aout_x, y):\n",
    "        self.visit_x = visit_x.astype(\"float32\")\n",
    "        self.aout_x  = aout_x.astype(\"float32\")\n",
    "        self.y       = y.astype(\"float32\")\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.visit_x[idx], self.aout_x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd3560c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def run_one_dataset(dataset_dir, dataset_name,\n",
    "                    device='cuda:0',\n",
    "                    batch_size=8, num_epochs=100, lr=1e-4,\n",
    "                    cap_len=None,              # kept for compat; fixed_len dominates\n",
    "                    patience=15,               # early stopping patience\n",
    "                    verbose=False, plot_curves=False,\n",
    "                    return_epoch_losses=False,   # ← 若为 True，则返回 test_loss_hist\n",
    "                    force_full_epochs=False,     # True 时无视早停，跑满 num_epochs\n",
    "                    collect_test_curve=False,    # ← 若为 True，逐 epoch 记录 TEST loss\n",
    "                    return_epoch_curves=False):  # ← 若为 True，返回 (train_curve, test_curve)\n",
    "    import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "    import torch, torch.nn as nn, torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "    # ---------- paths ----------\n",
    "    tsv_train_path = os.path.join(dataset_dir, f\"{dataset_name}_TRAIN_cleaned.tsv\")\n",
    "    tsv_test_path  = os.path.join(dataset_dir, f\"{dataset_name}_TEST_cleaned.tsv\")\n",
    "    aout_train_csv = os.path.join(dataset_dir, f\"{dataset_name}_Aout_train_k2.csv\")\n",
    "    aout_test_csv  = os.path.join(dataset_dir, f\"{dataset_name}_Aout_test_k2.csv\")\n",
    "    if not all(os.path.exists(p) for p in [tsv_train_path, tsv_test_path, aout_train_csv, aout_test_csv]):\n",
    "        print(f\"⚠ Skip {dataset_name}, missing files\"); return None\n",
    "\n",
    "    # ---------- read (OFFICIAL split; do NOT concat) ----------\n",
    "    tsv_tr = pd.read_csv(tsv_train_path, sep=\"\\t\", header=None)\n",
    "    tsv_te = pd.read_csv(tsv_test_path,  sep=\"\\t\", header=None)\n",
    "    csv_tr = pd.read_csv(aout_train_csv, header=None)\n",
    "    csv_te = pd.read_csv(aout_test_csv,  header=None)\n",
    "\n",
    "    y_tr_raw = tsv_tr.iloc[:,0].values\n",
    "    y_te_raw = tsv_te.iloc[:,0].values\n",
    "    visit_tr_raw = tsv_tr.iloc[:,1:].values.astype(\"float32\")\n",
    "    visit_te_raw = tsv_te.iloc[:,1:].values.astype(\"float32\")\n",
    "    aout_tr_raw_all  = csv_tr.iloc[:,1:].values.astype(\"float32\")\n",
    "    aout_te_raw_all  = csv_te.iloc[:,1:].values.astype(\"float32\")\n",
    "\n",
    "    # ---------- (1) determine TRAIN reference length ----------\n",
    "    tmp_train_clean, keep_tr0, _ = clean_and_pad_timeseries(\n",
    "        visit_tr_raw, min_len=8, cap_len=None, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=None\n",
    "    )\n",
    "    train_seq_len = tmp_train_clean.shape[1]\n",
    "\n",
    "    # ---------- (2) re-clean TRAIN/TEST with fixed_len=train_seq_len ----------\n",
    "    visit_tr_clean, keep_tr, _ = clean_and_pad_timeseries(\n",
    "        visit_tr_raw, min_len=8, cap_len=None, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=train_seq_len\n",
    "    )\n",
    "    y_tr = y_tr_raw[keep_tr]\n",
    "    aout_tr_raw = aout_tr_raw_all[keep_tr]\n",
    "\n",
    "    visit_te_clean, keep_te, _ = clean_and_pad_timeseries(\n",
    "        visit_te_raw, min_len=8, cap_len=None, pad_value=0.0,\n",
    "        per_sample_standardize=True, fixed_len=train_seq_len\n",
    "    )\n",
    "    y_te = y_te_raw[keep_te]\n",
    "    aout_te_raw = aout_te_raw_all[keep_te]\n",
    "\n",
    "    # ---------- (3) scaler: fit on TRAIN only ----------\n",
    "    scaler = StandardScaler().fit(aout_tr_raw)\n",
    "    aout_tr = scaler.transform(aout_tr_raw).astype(\"float32\")\n",
    "    aout_te = scaler.transform(aout_te_raw).astype(\"float32\")\n",
    "\n",
    "    # ---------- (4) labels ----------\n",
    "    classes = sorted(np.unique(y_tr))\n",
    "    Y_tr = label_binarize(y_tr, classes=classes).astype(\"float32\")\n",
    "    Y_te = label_binarize(y_te, classes=classes).astype(\"float32\")\n",
    "\n",
    "    # left tower to [B, L, 1]\n",
    "    visit_tr = visit_tr_clean[:, :, None]\n",
    "    visit_te = visit_te_clean[:, :, None]\n",
    "\n",
    "    # ---------- model dims ----------\n",
    "    seq_len1 = visit_tr.shape[1]; seq_len2 = 1\n",
    "    input_dim1 = visit_tr.shape[2]; input_dim2 = aout_tr.shape[1]\n",
    "    num_classes = Y_tr.shape[1]\n",
    "\n",
    "    hidden_dim1 = hidden_dim2 = hidden_dim3 = 16\n",
    "    num_heads = 2; num_layers = 2\n",
    "\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model = TwoTowerTransformer(\n",
    "        input_dim1, input_dim2,\n",
    "        hidden_dim1, hidden_dim2, hidden_dim3,\n",
    "        num_heads, num_layers,\n",
    "        num_classes,\n",
    "        seq_len1, seq_len2\n",
    "    ).to(device)\n",
    "    if torch.cuda.device_count() > 1 and str(device).startswith('cuda'):\n",
    "        model = nn.DataParallel(model, device_ids=[0,1])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(VisitDataset(visit_tr, aout_tr, Y_tr),\n",
    "                              batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "    test_loader  = DataLoader(VisitDataset(visit_te, aout_te, Y_te),\n",
    "                              batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # loss/opt/sched\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)\n",
    "\n",
    "    # early stopping on TRAIN loss only\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=15, delta=0.0):\n",
    "            self.patience = patience; self.delta = delta\n",
    "            self.counter = 0; self.best = None; self.stop = False\n",
    "        def step(self, train_loss):\n",
    "            if self.best is None:\n",
    "                self.best = train_loss\n",
    "                return False\n",
    "            if train_loss > self.best - self.delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.stop = True\n",
    "            else:\n",
    "                self.best = train_loss\n",
    "                self.counter = 0\n",
    "            return self.stop\n",
    "\n",
    "    es = EarlyStopping(patience=patience)\n",
    "\n",
    "    # logging\n",
    "    log_dir = os.path.join(dataset_dir, \"_twotower_logs\"); os.makedirs(log_dir, exist_ok=True)\n",
    "    log_file = open(os.path.join(log_dir, \"log_twotower.txt\"), \"w\", encoding=\"utf-8\")\n",
    "    train_loss_hist, test_loss_hist = [], []\n",
    "\n",
    "    # ---------- train + (optional) per-epoch TEST loss ----------\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for x1,x2,y in train_loader:\n",
    "            x1 = x1.to(torch.float32).to(device)\n",
    "            x2 = x2.to(torch.float32).to(device)\n",
    "            y  = y.to(torch.float32).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            o = model(x1,x2)\n",
    "            l = criterion(o,y)\n",
    "            l.backward(); optimizer.step()\n",
    "            total_loss += l.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_loss / max(1,len(train_loader))\n",
    "        train_loss_hist.append(avg_train_loss)\n",
    "        log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}\\n\")\n",
    "\n",
    "        # Per-epoch TEST loss (optional, 真·TEST loss)\n",
    "        if collect_test_curve:\n",
    "            model.eval()\n",
    "            t_total = 0.0\n",
    "            with torch.no_grad():\n",
    "                for x1,x2,y in test_loader:\n",
    "                    x1 = x1.to(torch.float32).to(device)\n",
    "                    x2 = x2.to(torch.float32).to(device)\n",
    "                    y  = y.to(torch.float32).to(device)\n",
    "                    o = model(x1,x2)\n",
    "                    l = criterion(o,y)\n",
    "                    t_total += l.item()\n",
    "            avg_test_loss = t_total / max(1, len(test_loader))\n",
    "            test_loss_hist.append(avg_test_loss)\n",
    "\n",
    "        # Early stop (only if not forcing full epochs)\n",
    "        stopped = es.step(avg_train_loss)\n",
    "        if stopped and not force_full_epochs:\n",
    "            break\n",
    "\n",
    "    log_file.close()\n",
    "\n",
    "    # ---------- TEST evaluation (run once after training) ----------\n",
    "    model.eval()\n",
    "    tout=[]; tlab=[]\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,y in test_loader:\n",
    "            x1 = x1.to(torch.float32).to(device)\n",
    "            x2 = x2.to(torch.float32).to(device)\n",
    "            o = model(x1,x2)\n",
    "            tout.append(o.detach().cpu().numpy()); tlab.append(y.numpy())\n",
    "    tout = np.concatenate(tout); tlab = np.concatenate(tlab)\n",
    "    try:\n",
    "        test_auc = roc_auc_score(tlab, tout, multi_class='ovr')\n",
    "    except:\n",
    "        test_auc = roc_auc_score(tlab, tout)\n",
    "    test_pred = np.argmax(tout, axis=1)\n",
    "    test_true = np.argmax(tlab, axis=1)\n",
    "    test_acc = accuracy_score(test_true, test_pred)\n",
    "    test_n = len(tlab)\n",
    "\n",
    "    print(f\"[{dataset_name}] TEST AUC={test_auc:.4f}, TEST ACC={test_acc:.4f}, n_samples={test_n}\")\n",
    "\n",
    "    # ---------- returns ----------\n",
    "    if return_epoch_curves:\n",
    "        # 返回 (train_curve, test_curve)；若未采集 test，则 test_curve=None\n",
    "        return test_auc, test_acc, test_n, train_loss_hist, (test_loss_hist if collect_test_curve else None)\n",
    "    elif return_epoch_losses:\n",
    "        # 只返回 TEST loss 曲线（若未采集，则为 None）\n",
    "        return test_auc, test_acc, test_n, (test_loss_hist if collect_test_curve else None)\n",
    "    else:\n",
    "        return test_auc, test_acc, test_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53f66614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def discover_datasets(root):\n",
    "    \"\"\"\n",
    "    Discover dataset subfolders that contain all four required files:\n",
    "      *_TRAIN_cleaned.tsv, *_TEST_cleaned.tsv, *_Aout_train_k2.csv, *_Aout_test_k2.csv\n",
    "    Returns: a sorted list of dataset names (folder names).\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    for name in sorted(os.listdir(root)):\n",
    "        subdir = os.path.join(root, name)\n",
    "        if not os.path.isdir(subdir):\n",
    "            continue\n",
    "        t_train = os.path.join(subdir, f\"{name}_TRAIN_cleaned.tsv\")\n",
    "        t_test  = os.path.join(subdir, f\"{name}_TEST_cleaned.tsv\")\n",
    "        a_train = os.path.join(subdir, f\"{name}_Aout_train_k2.csv\")\n",
    "        a_test  = os.path.join(subdir, f\"{name}_Aout_test_k2.csv\")\n",
    "        if all(os.path.exists(p) for p in [t_train, t_test, a_train, a_test]):\n",
    "            names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b24bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def run_all_datasets(root, device='cuda:0',\n",
    "                     batch_size=8, num_epochs=100, lr=1e-4,\n",
    "                     cap_len=None, verbose=False,\n",
    "                     patience=15):\n",
    "    import os, pandas as pd, numpy as np\n",
    "    from datetime import datetime\n",
    "\n",
    "    dataset_names = discover_datasets(root)\n",
    "    print(f\"Found {len(dataset_names)} datasets:\", dataset_names)\n",
    "\n",
    "    rows = []\n",
    "    for name in dataset_names:\n",
    "        out = run_one_dataset(\n",
    "            dataset_dir=os.path.join(root, name),\n",
    "            dataset_name=name,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            cap_len=cap_len,\n",
    "            patience=patience,     # 训练早停仅基于 Train Loss\n",
    "            verbose=verbose,\n",
    "            plot_curves=False,     # 不画图\n",
    "            return_epoch_losses=False,  # 不需要返回每个 epoch 的 loss\n",
    "            force_full_epochs=False     # 正式评估允许早停\n",
    "        )\n",
    "        if out is None:\n",
    "            continue\n",
    "\n",
    "        test_auc, test_acc, test_n = out\n",
    "        rows.append((name, float(test_auc), float(test_acc), int(test_n)))\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No dataset finished successfully.\")\n",
    "        return None\n",
    "\n",
    "    # 仅包含这四列，匹配你示例的格式\n",
    "    df = pd.DataFrame(rows, columns=[\"dataset\", \"test_auc\", \"test_acc\", \"n_samples\"]).sort_values(\"dataset\")\n",
    "\n",
    "    # 统计（可留作打印参考）\n",
    "    mean_auc = df[\"test_auc\"].mean()\n",
    "    mean_acc = df[\"test_acc\"].mean()\n",
    "    w_auc = (df[\"test_auc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "    w_acc = (df[\"test_acc\"] * df[\"n_samples\"]).sum() / df[\"n_samples\"].sum()\n",
    "\n",
    "    # 打印与示例一致的样式\n",
    "    print(\"\\n========== Summary (OFFICIAL TEST) ==========\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSimple mean: AUC = {mean_auc:.4f}, ACC = {mean_acc:.4f}\")\n",
    "    print(f\"Weighted (by samples): AUC = {w_auc:.4f}, ACC = {w_acc:.4f}\")\n",
    "\n",
    "    # 只保存 CSV（不写 txt）\n",
    "    summary_dir = os.path.join(root, \"_twotower_logs\")\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_csv = os.path.join(summary_dir, f\"summary_TEST_{ts}.csv\")\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df, (mean_auc, mean_acc), (w_auc, w_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1d838e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TwoTower] Discover 125 datasets\n",
      "[ACSF1] TEST AUC=0.9458, TEST ACC=0.7500, n_samples=100\n",
      "[Adiac] TEST AUC=0.9603, TEST ACC=0.5064, n_samples=391\n",
      "[AllGestureWiimoteX] TEST AUC=0.7795, TEST ACC=0.3730, n_samples=681\n",
      "[AllGestureWiimoteY] TEST AUC=0.8644, TEST ACC=0.4837, n_samples=645\n",
      "[AllGestureWiimoteZ] TEST AUC=0.7964, TEST ACC=0.3328, n_samples=685\n",
      "[ArrowHead] TEST AUC=0.8602, TEST ACC=0.6114, n_samples=175\n",
      "[BME] TEST AUC=0.9353, TEST ACC=0.6800, n_samples=150\n",
      "[Beef] TEST AUC=0.8875, TEST ACC=0.7000, n_samples=30\n",
      "[BeetleFly] TEST AUC=0.8100, TEST ACC=1.0000, n_samples=20\n",
      "[BirdChicken] TEST AUC=0.6700, TEST ACC=1.0000, n_samples=20\n",
      "[CBF] TEST AUC=0.9920, TEST ACC=0.9589, n_samples=900\n",
      "[Car] TEST AUC=0.8641, TEST ACC=0.6833, n_samples=60\n",
      "[Chinatown] TEST AUC=0.9911, TEST ACC=1.0000, n_samples=343\n",
      "[ChlorineConcentration] TEST AUC=0.6571, TEST ACC=0.5729, n_samples=3840\n",
      "[CinCECGTorso] TEST AUC=0.9941, TEST ACC=0.9196, n_samples=1380\n",
      "[Coffee] TEST AUC=0.9846, TEST ACC=1.0000, n_samples=28\n",
      "[CricketX] TEST AUC=0.8847, TEST ACC=0.5615, n_samples=390\n",
      "[CricketY] TEST AUC=0.9039, TEST ACC=0.5590, n_samples=390\n",
      "[CricketZ] TEST AUC=0.8772, TEST ACC=0.5923, n_samples=390\n",
      "[Crop] TEST AUC=0.9610, TEST ACC=0.6894, n_samples=16778\n",
      "[DiatomSizeReduction] TEST AUC=0.9785, TEST ACC=0.8856, n_samples=306\n",
      "[DistalPhalanxOutlineAgeGroup] TEST AUC=0.8080, TEST ACC=0.6768, n_samples=99\n",
      "[DistalPhalanxOutlineCorrect] TEST AUC=0.8086, TEST ACC=1.0000, n_samples=199\n",
      "[DistalPhalanxTW] TEST AUC=0.7640, TEST ACC=0.5213, n_samples=94\n",
      "[DodgerLoopDay] TEST AUC=0.8267, TEST ACC=0.4625, n_samples=80\n",
      "[DodgerLoopGame] TEST AUC=0.8016, TEST ACC=1.0000, n_samples=138\n",
      "[DodgerLoopWeekend] TEST AUC=0.9883, TEST ACC=1.0000, n_samples=138\n",
      "[ECG200] TEST AUC=0.9076, TEST ACC=1.0000, n_samples=100\n",
      "[ECG5000] TEST AUC=0.9223, TEST ACC=0.9387, n_samples=4500\n",
      "[ECGFiveDays] TEST AUC=0.9363, TEST ACC=1.0000, n_samples=861\n",
      "[EOGHorizontalSignal] TEST AUC=0.8542, TEST ACC=0.4669, n_samples=362\n",
      "[EOGVerticalSignal] TEST AUC=0.8272, TEST ACC=0.4061, n_samples=362\n",
      "[Earthquakes] TEST AUC=0.6398, TEST ACC=1.0000, n_samples=139\n",
      "[EthanolLevel] TEST AUC=0.5329, TEST ACC=0.2760, n_samples=500\n",
      "[FaceAll] TEST AUC=0.9456, TEST ACC=0.7284, n_samples=1690\n",
      "[FaceFour] TEST AUC=0.9453, TEST ACC=0.8272, n_samples=81\n",
      "[FacesUCR] TEST AUC=0.9535, TEST ACC=0.8156, n_samples=2050\n",
      "[FiftyWords] TEST AUC=0.9040, TEST ACC=0.6945, n_samples=455\n",
      "[Fish] TEST AUC=0.9628, TEST ACC=0.8343, n_samples=175\n",
      "[FordA] TEST AUC=0.5582, TEST ACC=1.0000, n_samples=1320\n",
      "[FordB] TEST AUC=0.5714, TEST ACC=1.0000, n_samples=810\n",
      "[FreezerRegularTrain] TEST AUC=0.9853, TEST ACC=1.0000, n_samples=2850\n",
      "[FreezerSmallTrain] TEST AUC=0.8306, TEST ACC=1.0000, n_samples=2850\n",
      "[Fungi] TEST AUC=0.9407, TEST ACC=0.6720, n_samples=186\n",
      "[GestureMidAirD1] TEST AUC=0.9637, TEST ACC=0.6308, n_samples=130\n",
      "[GestureMidAirD2] TEST AUC=0.9655, TEST ACC=0.5923, n_samples=130\n",
      "[GestureMidAirD3] TEST AUC=0.9165, TEST ACC=0.4154, n_samples=130\n",
      "[GesturePebbleZ1] TEST AUC=0.9648, TEST ACC=0.8256, n_samples=172\n",
      "[GesturePebbleZ2] TEST AUC=0.9733, TEST ACC=0.8228, n_samples=158\n",
      "[GunPoint] TEST AUC=0.8727, TEST ACC=1.0000, n_samples=150\n",
      "[GunPointAgeSpan] TEST AUC=0.9932, TEST ACC=1.0000, n_samples=316\n",
      "[GunPointMaleVersusFemale] TEST AUC=0.9982, TEST ACC=1.0000, n_samples=316\n",
      "[GunPointOldVersusYoung] TEST AUC=0.9863, TEST ACC=1.0000, n_samples=315\n",
      "[Ham] TEST AUC=0.8286, TEST ACC=1.0000, n_samples=105\n",
      "[HandOutlines] TEST AUC=0.9330, TEST ACC=1.0000, n_samples=370\n",
      "[Haptics] TEST AUC=0.7241, TEST ACC=0.4123, n_samples=308\n",
      "[Herring] TEST AUC=0.5405, TEST ACC=1.0000, n_samples=64\n",
      "[HouseTwenty] TEST AUC=0.9099, TEST ACC=1.0000, n_samples=119\n",
      "[InlineSkate] TEST AUC=0.6981, TEST ACC=0.3418, n_samples=550\n",
      "[InsectEPGRegularTrain] TEST AUC=0.8374, TEST ACC=0.6948, n_samples=249\n",
      "[InsectEPGSmallTrain] TEST AUC=0.8242, TEST ACC=0.7068, n_samples=249\n",
      "[InsectWingbeatSound] TEST AUC=0.9411, TEST ACC=0.6485, n_samples=1980\n",
      "[ItalyPowerDemand] TEST AUC=0.9923, TEST ACC=1.0000, n_samples=1028\n",
      "[LargeKitchenAppliances] TEST AUC=0.4651, TEST ACC=0.2353, n_samples=17\n",
      "[Lightning2] TEST AUC=0.8268, TEST ACC=1.0000, n_samples=61\n",
      "[Lightning7] TEST AUC=0.9516, TEST ACC=0.7260, n_samples=73\n",
      "[Mallat] TEST AUC=0.9965, TEST ACC=0.8648, n_samples=2345\n",
      "[Meat] TEST AUC=0.9662, TEST ACC=0.9167, n_samples=60\n",
      "[MedicalImages] TEST AUC=0.9174, TEST ACC=0.6408, n_samples=760\n",
      "[MelbournePedestrian] TEST AUC=0.9615, TEST ACC=0.7913, n_samples=2425\n",
      "[MiddlePhalanxOutlineAgeGroup] TEST AUC=0.5599, TEST ACC=0.5510, n_samples=147\n",
      "[MiddlePhalanxOutlineCorrect] TEST AUC=0.8665, TEST ACC=1.0000, n_samples=276\n",
      "[MiddlePhalanxTW] TEST AUC=0.6331, TEST ACC=0.3909, n_samples=110\n",
      "[MixedShapesRegularTrain] TEST AUC=0.9610, TEST ACC=0.8755, n_samples=2425\n",
      "[MixedShapesSmallTrain] TEST AUC=0.9410, TEST ACC=0.8202, n_samples=2425\n",
      "[MoteStrain] TEST AUC=0.9480, TEST ACC=1.0000, n_samples=1238\n",
      "[NonInvasiveFetalECGThorax1] TEST AUC=0.9948, TEST ACC=0.8855, n_samples=1965\n",
      "[NonInvasiveFetalECGThorax2] TEST AUC=0.9963, TEST ACC=0.9074, n_samples=1965\n",
      "[OSULeaf] TEST AUC=0.7692, TEST ACC=0.4876, n_samples=242\n",
      "[OliveOil] TEST AUC=0.9007, TEST ACC=0.4000, n_samples=30\n",
      "[PLAID] TEST AUC=0.9171, TEST ACC=0.7114, n_samples=537\n",
      "[PhalangesOutlinesCorrect] TEST AUC=0.8549, TEST ACC=1.0000, n_samples=596\n",
      "[Phoneme] TEST AUC=0.6328, TEST ACC=0.1304, n_samples=1894\n",
      "[PickupGestureWiimoteZ] TEST AUC=0.9209, TEST ACC=0.7200, n_samples=50\n",
      "[PigAirwayPressure] TEST AUC=0.4206, TEST ACC=0.0481, n_samples=208\n",
      "[PigArtPressure] TEST AUC=0.4596, TEST ACC=0.1202, n_samples=208\n",
      "[PigCVP] TEST AUC=0.4165, TEST ACC=0.0962, n_samples=208\n",
      "[Plane] TEST AUC=0.9974, TEST ACC=0.9619, n_samples=105\n",
      "[PowerCons] TEST AUC=0.9951, TEST ACC=1.0000, n_samples=180\n",
      "[ProximalPhalanxOutlineAgeGroup] TEST AUC=0.8272, TEST ACC=0.8019, n_samples=106\n",
      "[ProximalPhalanxOutlineCorrect] TEST AUC=0.9805, TEST ACC=1.0000, n_samples=121\n",
      "[ProximalPhalanxTW] TEST AUC=0.9117, TEST ACC=0.7724, n_samples=123\n",
      "[RefrigerationDevices] TEST AUC=0.6479, TEST ACC=0.4725, n_samples=364\n",
      "[Rock] TEST AUC=0.9444, TEST ACC=0.8400, n_samples=50\n",
      "[ScreenType] TEST AUC=0.6702, TEST ACC=0.4833, n_samples=60\n",
      "[SemgHandGenderCh2] TEST AUC=0.9023, TEST ACC=1.0000, n_samples=600\n",
      "[SemgHandMovementCh2] TEST AUC=0.7306, TEST ACC=0.3400, n_samples=450\n",
      "[SemgHandSubjectCh2] TEST AUC=0.9096, TEST ACC=0.7311, n_samples=450\n",
      "[ShakeGestureWiimoteZ] TEST AUC=0.9480, TEST ACC=0.6400, n_samples=50\n",
      "[ShapeletSim] TEST AUC=0.4738, TEST ACC=1.0000, n_samples=180\n",
      "[ShapesAll] TEST AUC=0.9291, TEST ACC=0.7150, n_samples=600\n",
      "[SmallKitchenAppliances] TEST AUC=0.6692, TEST ACC=0.8070, n_samples=57\n",
      "[SonyAIBORobotSurface1] TEST AUC=0.9571, TEST ACC=1.0000, n_samples=599\n",
      "[SonyAIBORobotSurface2] TEST AUC=0.8926, TEST ACC=1.0000, n_samples=945\n",
      "[StarLightCurves] TEST AUC=0.9812, TEST ACC=0.9392, n_samples=8236\n",
      "[Strawberry] TEST AUC=0.9802, TEST ACC=1.0000, n_samples=370\n",
      "[SwedishLeaf] TEST AUC=0.9873, TEST ACC=0.8656, n_samples=625\n",
      "[Symbols] TEST AUC=0.9797, TEST ACC=0.8824, n_samples=995\n",
      "[SyntheticControl] TEST AUC=0.9857, TEST ACC=0.9400, n_samples=300\n",
      "[ToeSegmentation1] TEST AUC=0.7454, TEST ACC=1.0000, n_samples=228\n",
      "[ToeSegmentation2] TEST AUC=0.8160, TEST ACC=1.0000, n_samples=130\n",
      "[Trace] TEST AUC=0.9914, TEST ACC=0.8500, n_samples=100\n",
      "[TwoLeadECG] TEST AUC=0.8121, TEST ACC=1.0000, n_samples=1139\n",
      "[TwoPatterns] TEST AUC=0.9265, TEST ACC=0.7842, n_samples=3957\n",
      "[UMD] TEST AUC=0.9721, TEST ACC=0.8611, n_samples=144\n",
      "[UWaveGestureLibraryAll] TEST AUC=0.9884, TEST ACC=0.9204, n_samples=3579\n",
      "[UWaveGestureLibraryX] TEST AUC=0.9290, TEST ACC=0.7603, n_samples=3238\n",
      "[UWaveGestureLibraryY] TEST AUC=0.9244, TEST ACC=0.6647, n_samples=3206\n",
      "[UWaveGestureLibraryZ] TEST AUC=0.9098, TEST ACC=0.6789, n_samples=3220\n",
      "[Wafer] TEST AUC=0.9992, TEST ACC=1.0000, n_samples=5935\n",
      "[Wine] TEST AUC=0.8395, TEST ACC=1.0000, n_samples=54\n",
      "[WordSynonyms] TEST AUC=0.8271, TEST ACC=0.5737, n_samples=638\n",
      "[Worms] TEST AUC=0.6908, TEST ACC=0.4026, n_samples=77\n",
      "[WormsTwoClass] TEST AUC=0.5324, TEST ACC=1.0000, n_samples=77\n",
      "[Yoga] TEST AUC=0.8217, TEST ACC=1.0000, n_samples=3000\n",
      "[TwoTower] Saved test-loss table to logs\\twotower_testloss.tsv with shape (60, 125)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7\n",
    "# === BATCH COLLECTOR (TwoTower): build logs/twotower_testloss.tsv ===\n",
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "NUM_EPOCHS = 60\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "dataset_names = discover_datasets(ROOT)\n",
    "print(f\"[TwoTower] Discover {len(dataset_names)} datasets\")\n",
    "\n",
    "# 行：epoch 1..NUM_EPOCHS；列：各数据集\n",
    "loss_table = pd.DataFrame(index=np.arange(1, NUM_EPOCHS+1),\n",
    "                          columns=dataset_names, dtype=float)\n",
    "loss_table.index.name = \"epoch\"\n",
    "\n",
    "for ds in dataset_names:\n",
    "    try:\n",
    "        out = run_one_dataset(\n",
    "            dataset_dir=os.path.join(ROOT, ds),\n",
    "            dataset_name=ds,\n",
    "            device='cuda:0',\n",
    "            batch_size=8,\n",
    "            num_epochs=NUM_EPOCHS,   # 固定 60\n",
    "            lr=1e-4,\n",
    "            cap_len=None,\n",
    "            patience=15,\n",
    "            verbose=False,\n",
    "            plot_curves=False,\n",
    "            force_full_epochs=True,        # 跑满 60，不早停\n",
    "            collect_test_curve=True,       # ← 开启逐 epoch 计算 TEST loss\n",
    "            return_epoch_losses=True       # ← 返回 test_loss_hist（不是 train）\n",
    "        )\n",
    "        if out is None:\n",
    "            print(f\"[SKIP] {ds} returned None\")\n",
    "            loss_table[ds] = np.nan\n",
    "            continue\n",
    "\n",
    "        # 期望结构：out = (test_auc, test_acc, test_n, test_loss_hist)\n",
    "        if len(out) != 4:\n",
    "            print(f\"[WARN] {ds}: unexpected return length={len(out)}\")\n",
    "        _, _, _, test_curve = out\n",
    "\n",
    "        if test_curve is None:\n",
    "            print(f\"[WARN] {ds}: test_curve is None (collect_test_curve=False?)\")\n",
    "            loss_table[ds] = np.nan\n",
    "            continue\n",
    "\n",
    "        # 对齐到固定行数（多了裁，少了补 NaN）\n",
    "        epoch_losses = (list(test_curve) + [np.nan]*NUM_EPOCHS)[:NUM_EPOCHS]\n",
    "        loss_table[ds] = epoch_losses\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {ds}: {e}\")\n",
    "        loss_table[ds] = np.nan\n",
    "\n",
    "out_path = os.path.join(LOG_DIR, \"twotower_testloss.tsv\")\n",
    "loss_table.to_csv(out_path, sep=\"\\t\", float_format=\"%.6f\")\n",
    "print(f\"[TwoTower] Saved test-loss table to {out_path} with shape {loss_table.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b21b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 datasets: ['ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY', 'AllGestureWiimoteZ', 'ArrowHead', 'BME', 'Beef', 'BeetleFly', 'BirdChicken', 'CBF', 'Car', 'Chinatown', 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'CricketX', 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction', 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'ECG200', 'ECG5000', 'ECGFiveDays', 'EOGHorizontalSignal', 'EOGVerticalSignal', 'Earthquakes', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain', 'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2', 'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint', 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7', 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian', 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain', 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2', 'OSULeaf', 'OliveOil', 'PLAID', 'PhalangesOutlinesCorrect', 'Phoneme', 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineAgeGroup', 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Quiet run with per-dataset one-line summaries + curves + early stopping:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcap_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 训练早停仅基于 Train Loss\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mrun_all_datasets\u001b[1;34m(root, device, batch_size, num_epochs, lr, cap_len, verbose, patience)\u001b[0m\n\u001b[0;32m     12\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m dataset_names:\n\u001b[1;32m---> 14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_one_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcap_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcap_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 训练早停仅基于 Train Loss\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 不画图\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_epoch_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 不需要返回每个 epoch 的 loss\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_full_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 正式评估允许早停\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 91\u001b[0m, in \u001b[0;36mrun_one_dataset\u001b[1;34m(dataset_dir, dataset_name, device, batch_size, num_epochs, lr, cap_len, patience, verbose, plot_curves, return_epoch_losses, force_full_epochs, collect_test_curve, return_epoch_curves)\u001b[0m\n\u001b[0;32m     82\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m; num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     84\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTwoTowerTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len2\u001b[49m\n\u001b[1;32m---> 91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     93\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model, device_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Jerry Kang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jerry Kang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jerry Kang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jerry Kang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "# Quiet run with per-dataset one-line summaries + curves + early stopping:\n",
    "result = run_all_datasets(\n",
    "    root=ROOT,\n",
    "    device='cuda:0',\n",
    "    batch_size=8,\n",
    "    num_epochs=100,\n",
    "    lr=1e-4,\n",
    "    cap_len=None,\n",
    "    verbose=False,\n",
    "    patience=15,        # 训练早停仅基于 Train Loss\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
